%!TEX root = Thesis.tex
\chapter{Our Approach}
\label{cha:our_approach}

\section{Overview} % (fold)
\label{sec:overview}
Unlike the works presented in the related works we were interested in integrating all stages of solving the problem such as perception, mapping model and action planning into one system. We further present an integrated framework for carrying out the task of detecting the parked cars, integrating occupancy information into a model that is suitable for further planning and planning for actions itself, when carrying out a decision of finding an unoccupied parking lot.
We will further describe the different steps that we have taken on this road as well as the connections between them.
% section overview (end)

\section{Perception} % (fold)
\label{sec:perception}
    In order to detect the positions of the parked cars we first needed to find a way for visual recognition of cars. To achieve this goal we looked at a number of approaches for object detection and classification.
    \begin{itemize}
        \item Haar-cascade detection as presented by \cite{violajones2001}
        \item Local binary patterns as presented by \todo{need a citation here}
        \item HOG descriptor based classifier by \cite{dalal2005}
    \end{itemize}
    However, whichever algorithm we use, we still need to make a transition from the image space to the 3D world. For each detected car we want to know precisely where is it situated with relation to the agent's current position.
    For this purpose we need to use depth information, which we may obtain from one of these sources:
    \begin{itemize}
        \item Sonar Based Depth
        \item Stereo Camera Based Depth
        \item Laser Range Finder Based Depth
    \end{itemize}
    We will further focus on the second two of them.
    As soon as we know a relative position of the detected car to the camera, we can move on to the next section - \nameref{sec:model}, where we stack many detections together in order to model the joint occupancy information.
    For now, let's look into some more details in each perception step:
    \subsection{HOG Detector} % (fold)
    \label{sub:hog_detector}
        After analyzing the good and the bad sides of these methods we decided to go with the HOG-based method as it outperforms the Haar-based method while providing us with shorter training times when comparing to the local binary patterns based methods.
        There are still some differences between our approach and the one presented by \cite{dalal2005}. These differences are due to the fact that we were interested in detecting cars, while the original paper is on detecting people.
        We were mainly focusing on detecting front/rear facing cars, but the approach is easily extended onto the side-view as-well.
        To be able to detect the front and read sides of the cars, we needed to switch the size of the hog descriptor window from $128 \times 64$ to $128 \times 128$.
        In the training faze we have considered around 1000 manually chosen $128 \times 128$ patches containing cars and around 8000 negative examples. Each HOG descriptor is then unfolded into a point in a $128 * 128 * bla-bla = ~8000$ \todo{size of HOG not defined} dimensional space.
    % subsection hog_classifier (end)

    \subsection{SVM Classifier} % (fold)
    \label{sub:svm_classifier}
        In order to afterwards carry out the decision in the test data, we have trained a linear SVM classifier on top of all HOG descriptors.
        A more complex decision boundary could of course also be used, but linear SVM, despite its simplicity, and remembering that visual detection was not the key contribution of this work, provides reasonable results in reasonable time.
        The test data detection is carried out in a cascade fashion as presented by \cite{violajones2001} via the sliding window approach. For each sliding window content we create a HOG descriptor which is then tested against the pre-trained SVM classifier in order to find out on which side of the decision boundary it is. If the current HOG belongs to the area where cars are then the current sliding window is a region of interest and contains a car.
        Each detection is then stored as a rectangle in the coordinates of the image it belongs to.
        This ends the visual detection part and allows us to move on to the 3D world.
    % subsection svm_classifier (end)

    \subsection{Depth Information} % (fold)
    \label{sub:depth_information}
        As we have pointed out before, we were mainly focusing on stereo cameras and laser range finders for depth acquisition. We did not consider using the sonar because it is by far not as reliable as a laser, while likewise needs a complicated setup. The stereo cameras, on the contrary, hardly need specific setup, but, on the down side, produce noisy information. However, for testing purposes, we decided to try using both - the stereo camera and the laser.
        \subsubsection{Stereo Camera} % (fold)
        \label{ssub:stereo_camera}
            In order to find the depth from the stereo camera, we need to first calculate the disparity image from left and right images taken from the video stream.
            Knowing the internal camera parameters we can then reconstruct the relative position of each pixel of the disparity image.
            We first find the distance along the camera $Z$ axis: $Z = \frac{fB}{d}$, where $f$ is the focal length of the camera, $B$ is the baseline and $d$ is the disparity.
            After $Z$ is determined we can focus on finding $X$ and $Y$ coordinates from the ordinary projection equations:
            $$X = \frac{uZ}{f}$$
            $$Y = \frac{vZ}{f}$$
            where $u$ and $v$ are the pixel location in the $2D$ image, $X, Y, Z$ is the real $3D$ position.
            When we have the $3D$ positions for every pixel in the images, we can combine this information with the visual detection part. From the previous part we have a region of interest in the image coordinates, which allows us to accumulate the depth of all pixels that fall into it. We can then analyze all of these values to find the distance to the car, that is contained in this particular region of interest.
            We considered taking either the mean or the median of the chosen values. Given, that the depth that comes from the stereo camera is quite noisy and that the depth values of the pixels originate on the surface of the car and therefore contain quite a big amount of variance, we picked a median as an option as it is the most resistant statistic, having a breakdown point of 50$\%$: so long as no more than half the data is contaminated, the median will not give an arbitrarily large result.
        % sub-subsection stereo_camera (end)
        \subsubsection{Laser Range Finder} % (fold)
        \label{ssub:laser_range_finder}
            Even though the stereo camera setup is a lot cheaper and easier to mount, it lacks robustness due to the noise that is present in disparity images and to some degree to the erroneous values that fall into the detected region of interest (such as around the car or in the glass parts of it). We therefore also tested a setup with a laser range finder. We used a EUROPA robot Obeliks which has a $270^\circ$ laser mounted approximately on the human knee level. This setup proves to be a lot more robust in terms of finding the exact $3D$ information about the position of the detected car.
            However, when using laser we need a different approach because there we only have images from a monocular camera and thus do not have a per-pixel association with the depth field the way we have it using the stereo camera.
            As the laser mounted on the robot provides us with $270^\circ$ span it is able to cover approximately all the space related to the image from the camera. We then need to find the beams, that span through the area covered by the image. Given the fact, that the camera is mounted on the same $Z$ axis as the laser this can be done in a straightforward fashion. We are interested in the beams, the numbers of which follows this law:
            $$ \{ beam_n | \forall n : \gamma_{start} < \alpha_{camera} + \alpha_{0} + \alpha_{n} < \gamma_{end} \}$$
            where $\alpha_{camera}$ is the angle of the camera with respect to the direction of the laser, $\gamma_{start}$ and $\gamma_{end}$ are the starting and ending angle of the detection bounding box in the image, $\alpha_{n}$ is the angle of the $n$-th beam in the laser frame.
            All the beam endpoints along with the direction of the beams provide us with consistent $3D$ information. In order to account for possible occlusions or mistakes we take the median of these values as a true position of the detected car.
            After finding the correct car position we may move on to the next part and aggregate occupancy information into a consistent spacio-temporal map.
        % sub-subsection laser_range_finder (end)
    % subsection depth_information (end)
% section perception (end)

\section{Model} % (fold)
\label{sec:model}
    In the previous sections we have shown the steps of visual detection of the cars and finding their real $3D$ world position.
    Whichever path we take in the previous step, now we have a number of real world coordinates, that need to be integrated into a consistent map. In this section we will present two approaches that we have chosen to target this task.
    \subsection{Occupancy Grids} % (fold)
    \label{sub:occupancy_grids}
        The first straightforward decision for mapping the occupancy is using the occupancy grid maps (see \nameref{cha:background}: \nameref{sec:occupancy_grids}). In this case we model each cell's occupancy as a static state Bayes filter.
        Let $z_t$ be an occupancy probability estimate of a cell in the environment in time $t$. Considering the probabilistic nature of occupancy of every distinct cell we integrate multiple measurement, taken in different times into one model. We achieve that by using the static state binary Bayes filter that integrates occupancy probability for each cell $i$ in $M$. Following the work of \cite{moravec1988}, we can compute a recursive update formula for $P(a^i | z_{1:t})$
        \begin{equation}
        \label{eq:static_state}
            P(a^i | z_{1:t}) = \left[1 + \frac{1 - P(a^i | z_{t})}{P(a^i | z_{t})} \frac{1 - P(a^i | z_{1:t-1})}{P(a^i | z_{1:t-1})} \frac{P(a^i)}{1 - P(a^i)} \right]^{-1}
        \end{equation}
        In order to gain efficiency, one can furthermore use the log-odds formulation of \cite{moravec1988}, so that the operations in \eqref{eq:static_state} are realized via addition and subtractions in the log-odds space.
    % subsection occupancy_grids (end)
    \subsection{Static State Binary Filter in Pre-defined Positions} % (fold)
    \label{sub:static_state_binary_filter_in_pre_defined_positions}

    % subsection static_state_binary_filter_in_pre_defin (end)
% section model (end)

\section{Action Planning} % (fold)
\label{sec:action_planning}

% section action_planning (end)
