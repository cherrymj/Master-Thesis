%!TEX root = Thesis.tex

\newcommand\Bx{x}
\newcommand\Bm{m}
\def\v{\vm{v}}
\newcommand\vm[1]{\bm{\mathrm{#1}}}
\renewcommand{\v}{{\mbox{a}^i}}
\newcommand{\z}{z_{t}}
\newcommand{\y}{z_{1:t-1}}

\chapter{In-vehicle parking space occupancy estimation and guidance system}
\label{cha:our_approach}

\section{Overview} % (fold)
\label{sec:overview}

In this chapter we describe in detail the three main components of our system:
visual car detection, creation of the map of parking lots capable to store
their occupancy information and a planner able to find a free parking lot in
the minimum expected time.

We start by a coarse outline of these different sections and show each of
these sub-parts in detail later in the chapter.

\begin{itemize}

\item \emph{Perception:}  We visually detect cars in the images taken from the
camera and estimate how far from the image plane are they situated by fusing
the detections with the depth data, received either from the stereo camera or
from the laser range finder (Section~\ref{sec:perception}).

\item \emph{Mapping:}  The positions of the detected cars are fused into one
global representation of the world observed by the agent. This representation
also contains occupancy information, perceived through multiple runs
(Section~\ref{sec:model}).

\item \emph{Planning:}  In the modeled environment we search for actions that
minimize the expected time spent on searching for a parking space and walking
from it to the goal (Section~\ref{sec:action_planning}).

\end{itemize}

% section overview (end)

\section{Perception} % (fold)
\label{sec:perception}

We start by motivating the choice of the visual detection framework. There is
a number of object detection methods to choose from. We focus on three
approaches, utilizing different visual features: Haar features, local binary
patterns (LBPs) and histograms of oriented gradients (HOGs). We further
provide a short comparison of these with relation to our problem.

\citet{violajones2001} introduce the Haar feature based object detection.
Their approach provides good detection rates when applied to face detection.
The main drawback of the proposed method for our setup is that it suffers from
the changes in the shape of the object, rotations, occlusions and from the
illumination changes. For the task of car detection it is important to allow
for a certain degree of deviation for the reason that the cars can differ in
shape and angle from which they are observed. They also can be occluded by
people walking in front of them or other cars. In addition to these
variations, we may also observe them in different lighting conditions.

Utilizing the local binary patterns (LBPs) as presented by~\citet{lbp2010}
addresses the illumination issues and overall boosts the detection rates and
performance. It is, however, similarly to Haar features, vulnerable to
rotation and shape changes.

Moreover, to the extent of our knowledge, LBP-based methods are slow on the
training stage. Despite, not being a crucial measure of an algorithm
performance, the training time is still an important argument, as it
influences the usability of the method, especially on implementation stage.

Considering the above, we apply the HOG-based detector following the work
of~\citet{dalal2005}. It relies on the gradient-based features and is
therefore invariant to illumination and brightness changes. Additionally, the
presented method stays robust to slight changes in rotation and shape.

\subsection{HOG Detector}\label{sub:hog_detector}

The HOG representation has several advantages. It captures edge or
gradient structure that is very characteristic of local shape, and it does
so in a local representation with an easily controllable degree of
invariance to local geometric and photometric transformations:
translations or rotations make little difference if they are much smaller
than the local spatial or orientation bin size.

Following~\citet{dalal2005}, we make use of the histograms of oriented
gradients placed in a dense grid on the query images. For each $8 \times 8$
pixel block in the image we form a histogram by assigning the gradient
orientations to 9 bins, that cover the angle of $180^{\circ}$. One HOG
descriptor consists of several such histograms.

To detect the front and rear sides of the cars, we set the size of the hog
descriptor window to $128 \times 128$ pixels. The idea behind this decision is
based on generally square-like shape of the cars, when seen from front or
rear. Therefore, the square drawn around the car contains higher percentage of
useful information in comparison to a rectangle. To detect the side of the
car, the descriptor window should be changed to $128 \times 64$ pixels
following the same logic.

These window sizes yield the number of the histograms that fit inside each
window which is the number of $8 \times 8$ pixel blocks contained in the
descriptor window. This results in 256 histograms for front/rear view and 128
histograms for the side view.

\begin{figure}[t]%
\centering
\subfloat{\includegraphics[width=0.6\textwidth]{pictures/detections.pdf}}
\subfloat{\includegraphics[width=0.4\textwidth]{pictures/svm.pdf}}
\caption{Illustration of HOG-based detection and clustering with SVM.}
\label{fig:det_to_svm}
\end{figure}

We form a vector of all the 9 bin values contained in each of 256 or 128
histograms contained in the detection window. This results in the vectors of
sizes respectively 2304 and 1152 values for front/rear and side view detection
windows. We view this vector as a point in the space of similar dimensionality
and seek for a hyperplane to divide these points into positive and negative
ones with the use of the support vector machines. This is illustrated in
Figure~\ref{fig:det_to_svm}.

% subsection hog_classifier (end)

\subsection{SVM Classifier}\label{sub:svm_classifier}

In order to carry out the decision in the test data, we train a linear Support
Vector Machines (SVM) classifier on top of all HOG descriptors. Alternatively,
a more complex decision boundary can be used, but linear SVM, despite its
simplicity, provides reasonable results. We show the detection performance in
the \nameref{cha:experimental_results} section.

\citet{svm} presented the support vector machines as a method to solve a two
class classification problem. For a dataset of points in $n$ dimensional space
SVM finds a hyperplane ($n-1$ dimensional plane) that optimally assigns them
to two classes maximizing the distance between the two resulting datasets. In
the linear SVM the decision boundary is linear.

SVM is a well known and arguably the most popular algorithm in binary
clustering, therefore many implementations are readily available. In this work
we make extensive use of libSVM library by~\citet{libSVM2011}.

Based on the trained classifier, we carry out the detection in a cascade
fashion via the sliding window approach. For each sliding window content we
create a HOG descriptor which we test against the pre-trained SVM classifier
to find out on which side of the decision boundary it is. If the current HOG
belongs to the side where the SVM has assigned the cars then the current
sliding window is a region of interest and contains a car. We store each
detection as a rectangle by storing the pixel positions of the corners of the
detection window around the car.

% subsection svm_classifier (end)

\subsection{Depth Information}\label{sub:depth_information}

In this section we describe in details how the depth information is acquired
and combined with the visual detections described in the previous sections.

\subsubsection{Stereo Camera}\label{ssub:stereo_camera}

In order to find the depth from the stereo camera, we calculate the disparity
image from left and right images taken from the video stream. Knowing the
internal camera parameters we reconstruct the relative position of each pixel
of the disparity image.

The distance along the camera $Z$ axis: $Z = \frac{fB}{d}$, where $f$ is the
focal length of the camera, $B$ is the baseline and $d$ is the disparity.
Given $Z$, we focus on finding $X$ and $Y$ coordinates from the projection
equations:

\begin{equation}
X = \frac{uZ}{f}
\hspace{10 mm}
Y = \frac{vZ}{f}
\end{equation}

where $u$ and $v$ are the pixel location in the $2D$ image and $X, Y, Z$ is
the $3D$ position that corresponds to the pixel $(u,v)$ in the camera frame.
$3D$ position that corresponds to every pixel in the image allows us to
combine this information with visual detections.

Considering, that each detection is stored in the form of a region of interest
in the image coordinates, we are able to accumulate the depth of all pixels
that fall into it to estimate the joint depth of the detected car.

To find the distance to the car, that is contained in this particular region
of interest we consider taking the median of all depth values that fall into
the given region of interest. These depth values contain high variance. The
first reason for this is that they originate on the surface of the car and the
variance represents the difference in the distance between different parts of
the car. The second reason comes from the method in which these depth values
were acquired. The depth map from the stereo camera setup proves to be noisy
in the urban environments with homogeneous regions like the sides or the hood
of the car. Moreover parts of the environment are seen through the windows of
the car, adding additional noise to measurements.

This high variance of the input data results in somewhat unreliable results.
We further describe the method that uses the laser range finder for the same
task of finding the relative position of the detected car to the camera.

% sub-subsection stereo_camera (end)
\subsubsection{Laser Range Finder}\label{ssub:laser_range_finder}

In order to use a laser range finder to detect the parked cars we used a
EUROPA robot Obelix which has a laser with an opening angle of $270^\circ$
mounted approximately on the human knee level. This setup proves to be more
robust in terms of finding the exact relative position of the detected car.

The laser mounted on the robot provides a sufficient opening angle to cover
the field of view of the camera. We search for the beams, that span through
the area covered by the image. The camera is mounted on the same vertical axis
with the laser. This yields the choice of the laser beams, that contain depth
values associated with the car:

\begin{equation}
Z = \{ \mbox{beam}_n | \forall n : \beta_{l} < \alpha_{camera} + \alpha_{0} + \alpha_{n} < \beta_{r} \}
\end{equation}

Here, $\alpha_{camera}$ is the angle at which the camera points as perceived
in the laser frame, $\beta_{l}$ and $\beta_{r}$ are the angles of left and
right margins of the detection bounding box in the camera frame, $\alpha_{n}$
is the angle of the $n$-th beam in the laser frame. $\alpha_{0}$ is the angle
of the first beam in the laser frame. In the setup we use, $\alpha_{0} =
-3\pi/4$ and $\alpha_{camera} = -\pi/2$.

The endpoints of the beams in $Z$ provide us with consistent depth
information. In order to account for possible occlusions or measurement errors
we take the median of these values.

% sub-subsection laser_range_finder (end)
% subsection depth_information (end)
% section perception (end)

\section{Model} % (fold)
\label{sec:model}

The methods mentioned above provide the cars positions relative to the camera.
Integrating this information with GPS measurements or SLAM-based pose
estimates, we calculate the positions of the detected cars in the world frame.
In this sections we present two approaches for storing these detections. We
also describe the approach to modeling the positions of the parking lots and
their occupancy probability.

\subsection{Occupancy Grids}
\label{sub:occupancy_grids}

The first approach that we consider utilizes the grid maps. Grid maps
partition the space into a grid of rectangular cells. Each grid contains
information about the corresponding area in the environment. We make use of
one particular realization of grid maps --- the \emph{occupancy grid maps} as
presented by~\citet{occupancy_grids}. Occupancy grid maps assume that each
grid cell is either occupied by an obstacle or free. In our case, each cell
stores the probability that the particular cell is occupied by a car.

The occupancy grid maps are an efficient approach for representing
uncertainty. Grid maps allow for detailed representation of an environment
without a predefined feature extractor. As they have the ability to represent
occupied space, empty space and unknown (unobserved) space, grid maps are well
suited for tasks such as path-planning, obstacle avoidance and exploration. In
contrast to most representations based on features, grid maps offer constant
time access to cells.

Let $P(\v)$ be an occupancy probability estimate of a cell $i$ in the
environment. Considering the probabilistic nature of occupancy of every
distinct cell we integrate multiple measurements, taken in different times
into one model. Following the work of~\citet{occupancy_grids}, we obtain an
update rule for $P(a_t\mid z_{1:t})$. We apply Bayes' rule and obtain

\begin{equation}
P(a_t \mid  z_{1:t} ) = \frac{P(z_t \mid a_t, \y) P(a_t \mid \y)}{P(z_t \mid \y)}.
\end{equation}
\noindent
We then compute the ratio
\begin{equation}
\frac{P(\v\mid z_{1:t})}{P(\neg\v \mid z_{1:t})}
=
\frac{P(\z \mid \v, \y)}{P(\z \mid \neg\v, \y)}   \frac{P(\v\mid \y)}{ P(\neg\v\mid \y)}.
\end{equation}
\noindent
Similarly, we obtain
\begin{equation}
\nonumber
\frac{P(\v\mid \z)}{P(\neg\v \mid \z)} = \frac{P(\z \mid \v)}{P(\z \mid \neg\v)}   \frac{P(\v)}{ P(\neg\v)},
\end{equation}
\noindent
which can be transformed to
\begin{equation}
\frac{P(\z \mid \v)}{P(\z \mid \neg\v)}
=
\frac{P(\v \mid \z)}{P(\neg\v \mid \z)}   \frac{P(\neg\v)}{ P(\v)}.
\end{equation}
\noindent

Applying the Markov assumption that the current observation is independent of
previous observations given we know that a cell contains a parked vehicle
gives

\begin{equation}
P(\z \mid \v, \y) = P(\z \mid \v),
\end{equation}
\noindent
and utilizing the fact that $P(\neg \v) = 1 - P(\v)$, we obtain
\begin{eqnarray}
\lefteqn{\frac{ P(\v \mid z_{1:t}) }{ 1 - P(\v \mid z_{1:t})} = } \nonumber \\
&&\frac{ P( \v \mid \z)}{ 1 - P(\v \mid \z)}   \frac{ P(\v \mid \y)}{ 1 - P(\v \mid  \y)}   \frac{ 1 - P(\v)}{ P(\v)}.
\end{eqnarray}

\noindent
This equation can be transformed into the following update formula:
\begin{eqnarray}
\label{eq:rec_update}
\hspace{-6mm}\lefteqn{P(\v\mid z_{1:t})=} \nonumber \\
&& \hspace{-10mm} \left[ 1 + \frac{1 - P(\v\mid \z)}{P(\v \mid \z)}
\frac{1 - P(\v \mid \y)}{P(\v \mid \y)}   \frac{P(\v)}{1 - P(\v)}  \right]^{-1}
\end{eqnarray}
\noindent

In order to gain efficiency, we furthermore use the log-odds \todo{add
log odds} formulation of~\citet{occupancy_grids}, so that the
operations in~\eqref{eq:rec_update} are realized via addition and
subtractions in the log-odds space.

In this approach we do not explicitly model the parking lots, but they are
described by groups of cells that were repeatedly observed to contain cars.

In order to fully define the equation given above, we need to define the
observation model $P(\v \mid z_t)$.

\begin{figure}[t]%
\centering
\subfloat[Bresenham based occupancy grid update from a single image with two detected cars.]{\includegraphics[width=0.3\textwidth]{pictures/testmap.png}\label{fig:maptest}}\hspace{2mm}
\subfloat[Observation model $P(a^i | z_t)$]{\includegraphics[width=0.65\textwidth]{pictures/observe_model.pdf}\label{fig:observation_model}}
\caption{}
\end{figure}

As presented in Figure~\ref{fig:observation_model}, we define the observation
model along the rays, that span from the camera position to the defined
$d_{\max}$ and along the defined for the camera field of view. In order to
find all the cells of the occupancy grid map that fall into the field of view
of the camera we compute left and right margin points. Following the work
of~\citet{bresenham1965} we search for the cells that form the frontier of the
camera's field of view that consists of the cells that lie on the line between
the above-mentioned left and right margin points. The margin points are shown
with red circles in Figure~\ref{fig:maptest}.

For each cell $q$ we carry out the same algorithm from the camera cell $c$ to
the query cell $c_i$. Whenever we encounter a cell containing a car, the next
cells within the size of the car are also updated as occupied. All the ones
that come afterwards are not updated as ``not visible''. These can be seen in
an example in Figure~\ref{fig:maptest} as gray ``tails'' behind the car
detections.

We formally define the observation model $P(\v \mid z_t)$ as follows:

\begin{equation}
\label{eq:observation_model}
P(a^i \mid z_t) = \begin{cases} P_{\mathrm{free}}, & \mbox{if } d(c_i) < d(\mathrm{detection}) \\ P_{\mathrm{occupied}}, & \mbox{if } d(\mathrm{detection}) < d(c_i) < d(\mathrm{detection}) + s_{\mathrm{car}} \\ P_{\mathrm{prior}}, & \mbox{if } d(c_i) > d(\mathrm{detection}) + s_{\mathrm{car}} \end{cases}
\end{equation}

where, $d(detection)$ is the distance to the detection point, $d(c_i)$ is the
distance to the $i_th$ cell, $s_{car}$ is the length of the car. We consider
all distances to be measured along the ray from the Bresenham algorithm.

Considering the current setup of the sensor that generates only few
detections, overwhelmed by a number of observations detecting cells as free,
we argue for setting $P{\mathrm{prior}}$ to 0.5, as we have no prior knowledge on the
occupancy of each cell, $P_{\mathrm{free}}$ to 0.45 to introduce only a slight update
when observing an unoccupied cell and $P_{\mathrm{occupied}}$ to 0.95 which emulates a
high certainty when detecting an occupied cell.


% subsection occupancy_grids (end)
\subsection{Static State Binary Filter in Pre-defined Positions}
\label{sub:static_state_binary_filter_in_pre_defined_positions}

However, occupancy grids have their drawbacks. One of them, especially for our
setup, is the discretization error. Whenever there are multiple detections of
the same car from different angles it can happen, that the detection is
assigned to different cells of the occupancy grids. This leads to the problem
of data association and therefore difficulties in creating a meaningful
accumulated map.

To avoid the discretization issues we make use of the pre-defined parking lot
positions. These can be set from manual measurements on the ground or from
aerial images. The rest of the theory stays untouched. For each parking lot we
model the occupancy probability likewise with the occupancy grid maps approach
via the static state binary Bayes filter as defined in~\eqref{eq:rec_update}.

The occupancy information can be measured and accumulated at any needed time,
for example on particular days of week.

The observation model, however, differs significantly from the one used in the
occupancy grip maps. Whenever we detect a parked car, we search for the
closest parking lot and assign the detection to it. The parking lot occupancy
is updated as occupied.

Additionally, we consider the detection of the parked cars to be divided in
sessions. For example, we consider a driving near every parking space in the
parking to be one session. The parking lots that were not updated as occupied
during the session are updated as free.

We explicitly stress, that this approach allows storing both spacial and
occupancy information in one distinct structure.

% subsection static_state_binary_filter_in_pre_defin (end)
% section model (end)

\section{Action Planning} % (fold)
\label{sec:action_planning}

In this section we formally define the action planning framework that searches
for a path to a free parking lot. We want to find the optimal one, but it
depends on the application what an optimal solution to this problem is. We
therefore define what we consider optimal in this work.

There are typically parts of the parking, which are better in the sense of
their distance to a common destination. This also means that the ``better''
parts of the parking lots are more likely to be occupied than the other, which
leads to a trade-off between driving straight to the closest-to-goal, but
likely occupied parking lot, and parking in a more distant area, most likely
free. It usually takes less time to drive a car than to walk by foot, but it
can take longer to find a free parking lot closer to the goal then walking
from a more distant one. Given the constraints described above, the framework
that we present focuses on finding the solution that minimizes the expected
time spent on finding a free parking lot and walking from the found one to the
goal.

We formally describe the parking as a graph. The states are defined as the set
of vertices of the graph $V$, each of which represents a point in space. The
graph also has a set of edges $E$ --- a function $V \rightarrow V$, that
corresponds to the actions that can be carried out from one state to another.

In the grid based situation, the states are the grid cells and the edges are
connecting the adjacent cells.

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{pictures/graph.pdf}
    \end{center}
    \caption{Graph based on the parking lot. The vertices correspond to the parking spaces. The edges between the vertices represent possibilities to reach one state from another. Every state is additionally connected to the goal state.}
    \vspace{-5mm}
    \label{fig:graph}
\end{figure}

In the case, where we explicitly map the parking lots --- they, along with
their spacial information, form the vertices of the graph. In this case we
define the edges so that they hold the spacial information of the parking.

In both cases all vertices are connected to the goal state. The probability of
the action that allows the agent to follow such edges relies on the occupancy
probability of the related parking lot and is formally defined later.

We also define the set of all actions $A$ that can be carried out in any
state. It consists of five distinct actions: ``up'': $\uparrow$, ``down'':
$\downarrow$, ``left'': $\leftarrow$, ``right'': $\rightarrow$ and ``park''.
Formally:
\begin{eqnarray}
A_{\mathrm{move}} = \{ \uparrow, \downarrow, \leftarrow, \rightarrow \} \\
a_{\mathrm{park}} = \mbox{``park''} \\
A = A_{\mathrm{move}} \cup a_{\mathrm{park}}
\label{eq:actions}
\end{eqnarray}

As presented in the work of~\citet{tipaldiICRA11}, Markov decision processes
(MDPs) provide a way to maximize joint rewards instead of greedily going for
the best possible goal. We follow similar approach, defining rewards and
transition probabilities based on the environment specific for our problem.

Following the work of~\citet{bellman1957} we define the MDP by the initial
state $S_0$, transition function $T(s \mid s', a)$ and a reward function $R(s,
s', a)$.

Here, $T(s \mid s', a)$ denotes the probability of reaching state
$s$ if and action $a$ is carried out in state $s'$. The transitions in the
model are assumed to be Markovian. That is, the probability of reaching $s$
from $s'$ depends only on $s'$ and not on the history of earlier states.

For four actions that describe driving around the parking, we consider the
probability of moving from any state to the next one (if possible) to be equal
to 1, while the probability of parking in a particular state is equal to the
occupancy probability of the state:
\begin{eqnarray}
\forall (s, s') \in E, \forall a \in A_{\mathrm{move}} : P(s \mid s', a) = 1 \\
\forall s \in V, (s,s_{\mathrm{goal}}) \in E : P(s \mid s_{\mathrm{goal}}, a_{\mathrm{park}}) = P(\mathrm{free})
\end{eqnarray}
where $P(\mathrm{free})$ is a probability of a parking lot to be free, observed
through a period of time. To be consistent, we set the remaining probabilities
to the values that guarantee that $\forall s, s' \in V, (s, s') \in E, \forall
a \in A: \sum_{s}P(s \mid s', a) = 1$. This ensures that in each state the
outcome of each action the agent can take is defined. It resolves to staying
in the same state when trying to carry out an unavailable or improbable action.

For example: it is clear, that staying in the left most corner of the parking
lot and carrying an action to go left, should result in staying in the same
spot and while carrying out an action of parking, with the probability of
$0.6$ there should be a $0.4$ chance to stay in the same place.

The function $R(s, s', a)$, likewise, defines the reward for reaching state $s$
from state $s'$ carrying out action $a$. $S_0$ denotes the state in which we
initially put the agent.

The motivation behind the rewards is based on the time it takes the agent to
carry out an action. We assume, that the agent needs time for driving from one
state $s$ to another $s'$. If he cannot carry out the action and is forced to
stay in the same position, he regardlessly loses time.

The time $T$ it takes the agent to drive from any arbitrary state $s$ to an
adjacent one $s'$ depends on the distance between these states and on the
agent's driving speed $v_{\mathrm{drive}}$ and can be defined as:

\begin{equation}
\label{eq:time_to_drive}
\forall s,s' \in V, (s, s') \in E: T_{\mathrm{drive}}^{s,s'}
= \frac{\sqrt{(s - s') {(s - s')}^T}}{v_{\mathrm{drive}}}
\end{equation}

The cost $R(s, s', a)$ of moving from state $s'$ to state $s$ carrying out
action $a$ grows with the time $T_{\mathrm{drive}}^{s,s'}$ the agent needs to cover the path. We therefore
define a negative reward for each action in $A_{\mathrm{move}}$ between any two
adjacent states as follows:

\begin{equation}
\forall s,s' \in V, \forall (s, s') \in E, \forall a \in A_{\mathrm{move}}: R(s, s', a) = -T_{\mathrm{drive}}^{s,s'}
\end{equation}

In addition to these costs we penalize staying in one place. This cost can
also be seen as the cost of failure as in our system the agent stays in the
same state only if he fails to carry out an optimal action. We define this
cost to be equal to a constant provided beforehand. To show the impact of this
cost let us consider an example. The agent stays in an arbitrary state and
thinks that the best decision in this state is to park. However, when he
observes the parking space --- it is occupied which results in the agent
staying in the same place and as a result, getting the negative reward. Given
the fact, that all rewards in our system are time based this cost virtually
results in losing a pre-defined amount of time.

We also define the rewards that the agent receives when he successfully
carries out parking actions. These rewards are likewise time based. They form
a decreasing linear function in order to guarantee that the state closer to
the destination gets a bigger reward than the one situated further. We define
the reward to be $r_{\max} - r_s$, where $r_s = T_{\mathrm{walk}}^{s,s_{\mathrm{goal}}}$ is the
time that the agent needs to walk from an arbitrary state $s \in \mathbb{R}^2$
to the goal, $r_{\max} = \max_{s \in V}r_s$ is the greatest of those
distances. We define $T_{\mathrm{walk}}^{s,s_{\mathrm{goal}}}$ similarly to
\eqref{eq:time_to_drive}: $T_{\mathrm{walk}}^{s,s_{\mathrm{goal}}} = \frac{\sqrt{(s - s_{\mathrm{goal}})
{(s - s_{\mathrm{goal}})}^T}}{v_{\mathrm{walk}}}$, where $v_{\mathrm{walk}}$ is the agent's walking
speed. Formally:

\begin{equation}
\forall s \in V, (s, s_{\mathrm{goal}}) \in E : R(s, s_{\mathrm{goal}}, a_{\mathrm{park}}) = r_{\max} - r_s
\end{equation}

The proposed reward function guarantees, that the agent tries to find a trade-
off between shorter driving and shorter walking paths, weighted by the
occupancy probability of the parking lots.

Transition function and reward function are necessary to define the utility
function, which is a measure of how good the state is in the long run:

\begin{equation}
U^{\pi}(s) = E\left[\sum_{t=0}^{\infty} \gamma^t R(s_t) \mid \pi,s_0 = s \right]
\end{equation}

We choose the optimal action in each state using the Maximum Utility
Principle, that is, choose: the action that maximizes the expected utility of
the subsequent state:

\begin{equation}
\label{lbl:optimal_policy}
\pi^*(s) = \argmax_a \sum_{s'} T(s \mid s', a)U(s')
\end{equation}

Provided, that the utility of the state is the expected sum of discounted
rewards from that point onwards, we define the utility via Bellman Equation:

\begin{equation}
\label{lbl:bellman_equation}
U(s) = R(s) + \gamma \max_a \sum_{s'}T(s \mid s', a)U(s')
\end{equation}

To solve the MDP we make use of the policy iteration algorithm which we
briefly outline here.

The policy iteration algorithm alternates the following two steps, beginning
from some initial policy $\pi_0$,

\begin{itemize}
    \item \emph{Policy evaluation:} given a policy $\pi_i$,
    calculate $U_i = U^{\pi_i}$, the utility of each state if $\pi_i$, were to be
    executed.
    \item \emph{Policy improvement:} Calculate a new policy
    $\pi_{i+l}$, using one-step look-ahead based on $U_i$ (as in~\eqref{lbl:optimal_policy}).
\end{itemize}

The algorithm terminates when the policy improvement step yields no change in
the utilities. The given method can be shown to converge in $O(n^3)$ where $n$
is the number of vertices in the graph.

This solution to the MDP guarantees to find a route that maximizes the rewards
while searching for the free parking lot and walking from the found one to the
goal. Given the nature of the reward function this approach can be seen as
minimizing the time loss.

\todo{explicitly state about replanning}

The agent carries out an optimal strategy to the point when he decides to
park. Let us imagine for a moment that the parking lot at which the agent
wants to park is occupied. From the point of view of the MDPs the optimal
decision would be to wait and try once again, which is clearly a suboptimal
decision. Therefore, we also utilize the observations of the world.

Whenever we move past a parking lot, we check if it is occupied and update
the occupancy probability in the related state to 1 or 0 respectively.
This allows for carrying out a decision based on the better background.

Because the graph is updated, we carry out the policy iteration algorithm
once more to define the new optimal actions for each state.

We repeat this re-planning step until we find a free parking lot.

% section action_planning (end)
